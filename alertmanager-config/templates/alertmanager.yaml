#alertmanager configuration secret
#To-do: Pipeline this
global:
  resolve_timeout: 5m
  smtp_smarthost: {{ .Values.smtp_smarthost | squote }}
  smtp_from: {{ .Values.smtp_from | squote }}
  smtp_auth_username: {{ .Values.smtp_auth_username | squote }}
  smtp_auth_identity: {{ .Values.smtp_auth_username | squote }}
  smtp_auth_password: {{ .Values.smtp_auth_password | squote }}
  smtp_require_tls: true
receivers:
- name: email
  email_configs:
  - to: {{ .Values.email_to | squote }}
    send_resolved: true
    html: {{`'{{ template "email.email.html" . }}'`}}
- name: test_email
  email_configs:
  - to: {{ .Values.test_email_to | squote }}
    send_resolved: true
    html: {{`'{{ template "email.email.html" . }}'`}}
- name: cloud_slack
  slack_configs: 
  - api_url: {{ .Values.cloud_slack_incoming_webhook | squote }}
    channel: '#prometheus-alerts'
    send_resolved: true
    color: {{`'{{ template "slack.slackcustom.color" . }}'`}}
    title: {{`'{{ template "slack.slackcustom.title" . }}'`}}
    text: {{`'{{ template "slack.slackcustom.text" . }}'`}}
- name: black_hole #Empty default receiver
# To do: come up with a low/info-severity route, or else configure Prometheus to not send low/info-severity to AlertManager.
# Ref: https://github.com/prometheus/alertmanager/issues/428#issuecomment-468952018
route:
  group_by: ['alertname', 'namespace']
  group_interval: 10m
  group_wait: 2m
  # repeat_interval cannot be greater than alertmanagerSpec retention
  repeat_interval: 1w
  receiver: black_hole
  routes:
  # Slack alerts
  - match_re:
      severity: major|urgent
    continue: true
    receiver: cloud_slack
  # Email alerts
  - match:
      severity: urgent
    receiver: email
  # Test email alerts
  - match:
      alertname: 'off'
    receiver: test_email
    repeat_interval: 10m
inhibit_rules:
  # Don't alert generic NodeNotReady when there is a known cause
  - target_match:
      alertname: 'NodeNotReady'
    source_match_re:
      alertname: '(NodeDiskPressure|NodeMemoryPressure|NodePIDPressure|NodeMemoryPressure|NodeNetworkUnavailable)'
    equal: ['node']
  - target_match:
      alertname: 'NodeLowPodCapacity'
    source_match:
      alertname: 'NodePodsFull'
    equal: ['node']
  - target_match:
      alertname: 'NodepoolLowPodCapacity'
    source_match:
      alertname: 'NodepoolPodsFull'
    equal: ['label_agentpool']
  - target_match:
      severity: 'medium'
    source_match:
      severity: 'high'
    equal: ['alertname', 'namespace']
  # To do: have pressure conditions inhibit corresponding low alerts once thresholds are stablished

templates:
- '*.tmpl'
- '*.html'
