#alertmanager configuration secret
#To-do: Pipeline this
global:
  resolve_timeout: 5m
  smtp_smarthost: {{ .Values.smtp_smarthost | squote }}
  smtp_from: {{ .Values.smtp_from | squote }}
  smtp_auth_username: {{ .Values.smtp_auth_username | squote }}
  smtp_auth_identity: {{ .Values.smtp_auth_username | squote }}
  smtp_auth_password: {{ .Values.smtp_auth_password | squote }}
  smtp_require_tls: true
receivers:
- name: email
  email_configs:
  - to: {{ .Values.email_to | squote }}
    require_tls: true
- name: cloud_slack
  slack_configs: 
  - api_url: {{ .Values.cloud_slack_incoming_webhook | squote }}
    channel: '#prometheus-alerts'
    send_resolved: true
    title: {{`'{{ template "slack.slackcustom.title" . }}'`}}
    text: {{`'{{ template "slack.slackcustom.text" . }}'`}}
- name: black_hole #Empty default receiver
# To do: come up with a low/info-severity route, or else configure Prometheus to not send low/info-severity to AlertManager.
# Ref: https://github.com/prometheus/alertmanager/issues/428#issuecomment-468952018
route:
  group_by: ['alertname', 'namespace']
  group_interval: 24h
  group_wait: 2m
  # repeat_interval cannot be greater than alertmanagerSpec retention which is 120h by default
  repeat_interval: 120h
  receiver: black_hole
  routes:
  - match_re:
      severity: medium|high
    continue: true
    receiver: cloud_slack
  # Email alerts WIP/testing
  - match:
      # severity: high
      alertname: 'off'
    receiver: email
# Don't alert generic NodeNotReady when there is a known cause
inhibit_rules:
  - target_match:
      alertname: 'NodeNotReady'
    source_match_re:
      alertname: '(NodeDiskPressure|NodeMemoryPressure|NodePIDPressure|NodeMemoryPressure|NodeNetworkUnavailable)'
templates:
- '*.tmpl'
